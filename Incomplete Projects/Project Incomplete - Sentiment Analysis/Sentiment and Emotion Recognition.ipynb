{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96ee3d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i didnt feel humiliated</th>\n",
       "      <th>sadness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             i didnt feel humiliated  sadness\n",
       "0  i can go from feeling so hopeless to so damned...  sadness\n",
       "1   im grabbing a minute to post i feel greedy wrong    anger\n",
       "2  i am ever feeling nostalgic about the fireplac...     love"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing Libraries for ML\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,roc_auc_score, roc_curve\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Importing Data\n",
    "\n",
    "df = pd.read_csv('sentiment.txt', delimiter = \";\", )\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59a4f5c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15999, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa51c894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ive been feeling a little burdened lately wasn...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15994</th>\n",
       "      <td>i just had a very brief time in the beanbag an...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15995</th>\n",
       "      <td>i am now turning and i feel pathetic that i am...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15996</th>\n",
       "      <td>i feel strong and good overall</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15997</th>\n",
       "      <td>i feel like this was such a rude comment and i...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15998</th>\n",
       "      <td>i know a lot but i feel so stupid because i ca...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15999 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  emotion\n",
       "0      i can go from feeling so hopeless to so damned...  sadness\n",
       "1       im grabbing a minute to post i feel greedy wrong    anger\n",
       "2      i am ever feeling nostalgic about the fireplac...     love\n",
       "3                                   i am feeling grouchy    anger\n",
       "4      ive been feeling a little burdened lately wasn...  sadness\n",
       "...                                                  ...      ...\n",
       "15994  i just had a very brief time in the beanbag an...  sadness\n",
       "15995  i am now turning and i feel pathetic that i am...  sadness\n",
       "15996                     i feel strong and good overall      joy\n",
       "15997  i feel like this was such a rude comment and i...    anger\n",
       "15998  i know a lot but i feel so stupid because i ca...  sadness\n",
       "\n",
       "[15999 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.rename(columns={\"i didnt feel humiliated\":\"sentence\", \"sadness\":\"emotion\"})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a321d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sadness', 'anger', 'love', 'surprise', 'fear', 'joy'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"emotion\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d9e0c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ive been feeling a little burdened lately wasn...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15994</th>\n",
       "      <td>i just had a very brief time in the beanbag an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15995</th>\n",
       "      <td>i am now turning and i feel pathetic that i am...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15996</th>\n",
       "      <td>i feel strong and good overall</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15997</th>\n",
       "      <td>i feel like this was such a rude comment and i...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15998</th>\n",
       "      <td>i know a lot but i feel so stupid because i ca...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15999 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  emotion\n",
       "0      i can go from feeling so hopeless to so damned...        1\n",
       "1       im grabbing a minute to post i feel greedy wrong        2\n",
       "2      i am ever feeling nostalgic about the fireplac...        3\n",
       "3                                   i am feeling grouchy        2\n",
       "4      ive been feeling a little burdened lately wasn...        1\n",
       "...                                                  ...      ...\n",
       "15994  i just had a very brief time in the beanbag an...        1\n",
       "15995  i am now turning and i feel pathetic that i am...        1\n",
       "15996                     i feel strong and good overall        6\n",
       "15997  i feel like this was such a rude comment and i...        2\n",
       "15998  i know a lot but i feel so stupid because i ca...        1\n",
       "\n",
       "[15999 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"emotion\"] = df[\"emotion\"].replace({'sadness':1, 'anger':2, 'love':3, 'surprise':4, 'fear':5, 'joy':6})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f36603a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"sentence\"].tolist()\n",
    "Y = df[\"emotion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9795b19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocessing_text(string):\n",
    "    pre1 = re.sub(\"[^a-zA-Z]\",\" \",string)\n",
    "    pre2 = pre1.lower()\n",
    "    pre3 = pre2.split()\n",
    "    pre4 = set([lemmatizer.lemmatize(str(x)) for x in pre3])\n",
    "    stop = set(stopwords.words(\"english\"))\n",
    "    pre5 = [w for w in pre4 if not w in stop] \n",
    "    \n",
    "    preprocessed = \" \".join(pre5)\n",
    "    \n",
    "    return preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15ed9702",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [preprocessing_text(i) for i in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42f1fd67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['someone care awake around hopeless go hopeful feeling damned',\n",
       " 'grabbing minute im feel greedy wrong post',\n",
       " 'ever feeling nostalgic know fireplace property still',\n",
       " 'grouchy feeling',\n",
       " 'little feeling ive sure wa lately wasnt burdened',\n",
       " 'feel funny lot asleep time like milligram amount faster recommended also ive taking fallen',\n",
       " 'feel year teenager old man confused life jaded',\n",
       " 'petronas feel ha year profit huge performed made well',\n",
       " 'romantic feel',\n",
       " 'something seeing feel make suffering like mean',\n",
       " 'feel expect encounter type spiritual running experience divine',\n",
       " 'feel year think easiest dissatisfied time',\n",
       " 'energy feel thirsty low',\n",
       " 'feel precious proto let time sympathy find sign possible write corner writer contract trying alone publishing point life little immense general agent',\n",
       " 'feel side anxiety reassured',\n",
       " 'embarrassed didnt feel really',\n",
       " 'feel pathetic pretty time',\n",
       " 'barbie vintage started sixty began child collection doll feeling sentimental',\n",
       " 'feel value work put every unit skeptical compromised',\n",
       " 'feel anything rejected without anyone saying irritated',\n",
       " 'god feel pour strategy list five form grateful completely journal overwhelmed two feeling thing heart letter grounded help end',\n",
       " 'feeling wa delighted amused',\n",
       " 'lifeline encouragement chai able help glad support feeling wa great',\n",
       " 'already feel dont fucked morning eat though usually like',\n",
       " 'love people effect stressed around bitter best wish angry longer tolerate turned bm person life feeling particularly ha always fact kind still',\n",
       " 'someone kitchen im painting feel el inhibited picture like',\n",
       " 'feel become overwhelmed defeated',\n",
       " 'lenghth explain feel need etc like pp body kinda measure appalled wide',\n",
       " 'child dead feel chicken superior grieving',\n",
       " 'feeling pencil elegant giddy fitted get perfectly skirt',\n",
       " 'feeling distressed remember acutely day',\n",
       " 'day left seen past couple company heard feeling impressed read',\n",
       " 'hill course climbed frustrated pretty ever hampered made id factor entirely feeling paced much wrong ha never dent day',\n",
       " 'physical someone morally feel scenario contact totally real subsequently safe sex close emotionally would imagine life accepted prolonged expected acceptable enough connected',\n",
       " 'feel anything sure make content would',\n",
       " 'feeling creative need',\n",
       " 'someone feel splendid however step le self causing want something away know',\n",
       " 'feel gentleman christmas writing gift ask greedy bit elderly mild greed rude',\n",
       " 'someone feel word need protected find season safe small',\n",
       " 'inspiration feel hope handmade adventure inspired plan share creation everyday story also life traveling',\n",
       " 'already get christmas festive spurring started tree got book two feeling sure',\n",
       " 'worn brave perfect dont im pale want little feeling ive concealer day',\n",
       " 'strongly feel fun poke passionate make u jerk decides',\n",
       " 'already year get cow discouraged peter paul cant way pay feeling afford robbing wa',\n",
       " 'something feeling thing different new wa listless need',\n",
       " 'feel wanted sam felt lost time paragraph many special sane mind said used worry book tell know wa guarantee reading still',\n",
       " 'first let mine go sad home want feeling accepted',\n",
       " 'denmark trip boat',\n",
       " 'feeling stopped hot began cold',\n",
       " 'dough feel sure perfect make need',\n",
       " 'little feeling morning discouraged found',\n",
       " 'spoiled selfish feel',\n",
       " 'go little bit feeling stymied might wrote somewhere wa story unsure unintended',\n",
       " 'good skin valentine bag scissor u awake gorgeous unkind daddy gerling jacynthe sister rupaul fed feelin kayle lookin look cryin qaf sun',\n",
       " 'realm know feel basically fiction fake like science',\n",
       " 'feel providing ill roof need dad possible leave think give hate unwelcome asshole live excuse living make',\n",
       " 'situation feeling pleasantly keep new ease supportiveness surprised also',\n",
       " 'anymore cumming yes misspelt fall activity begin part sexual coming feeling vigorous day',\n",
       " 'mom loving graceful feel rob nurture heal warm smile time',\n",
       " 'feel talk stiff one think brother popular extremely law',\n",
       " 'ate throughout feel cellular tingle level could place almost gentle healing feeling taking wa',\n",
       " 'feel young like girl beautiful trend implant thin pressured butt depending rejuvenated',\n",
       " 'people time began tortured hallucination figure week moving feeling several vibration sound',\n",
       " 'week feel detox nearly amazing finished',\n",
       " 'selfish feel back answered others former considered prayer may read never asked deserve post',\n",
       " 'pain child becomes know feel parent violent enraged',\n",
       " 'something supposed feeling roller emotion coaster wa unpleasant coming',\n",
       " 'truth lately need uncertainty shared suppose havent faith doubt faithful dwelled feeling ive',\n",
       " 'feeling bought brave wa clearly makeup',\n",
       " 'feeling c miserable mum earth proudest also',\n",
       " 'michelle feel love anyone u family embarrassed around matter ballistic go else figure',\n",
       " 'feel bomb see print think f sex journal story necessary reassured necessarily',\n",
       " 'feel talking put ovary aching like',\n",
       " 'picture feel much many mostly unimportant stuff took chris like',\n",
       " 'finding blamed well result unsuitable sit im book ready given tired feeling wa image',\n",
       " 'achievement feel stretch time mxm canvas successfully manage usage brief use future worthwhile wa money',\n",
       " 'feeling thing one think important nay',\n",
       " 'woman feel young completely girl alive honored talented influence fully beautiful',\n",
       " 'firey angered feel',\n",
       " 'feel piece miserable garbage like',\n",
       " 'feel anything need like dont list miss would leanne thought make appalled',\n",
       " 'drove dannika corner turned bit little feeling greeted wa school rushed',\n",
       " 'feeling furious shooter hellip remember',\n",
       " 'learned since thing feel happy many excited',\n",
       " 'feel around horse safe quiet handled frequently relaxed show trusting people',\n",
       " 'left done couple december feeling thing smug make start',\n",
       " 'feel could ok realize think make everything u',\n",
       " 'feel struggling finding work wa worthless time',\n",
       " 'feel able dark terrified bed least lay',\n",
       " 'mom ever arm feel around airport ready wa meet supportive',\n",
       " 'strange feeling im entire mood ha bitter today guess day',\n",
       " 'present bringing colour mum passed brother five form car away flying exam accident wa involved',\n",
       " 'go letting towards feel ha animosity anyone wronged',\n",
       " 'feel word talk decided dog go cannot supportive home know read understand emotion',\n",
       " 'throwing away shitty piece feel shit paper like',\n",
       " 'im feel wryly banal starting error life turning amused comedy',\n",
       " 'find feel body every vital people want beautiful',\n",
       " 'owner property feel attorney victimized association manager hear',\n",
       " 'feel year first another goodbye pretty time like hey theyre excited say im sad fam heartless also cry bitch flying know spend country',\n",
       " 'loving wont feel going lily let month short child lasted cry opportunity little wa',\n",
       " 'good im feel fitting alba',\n",
       " 'ton im feel catching excited like',\n",
       " 'good feel word back see honored also look know help start',\n",
       " 'technical obtain work feel motivated saw ashamed heartedly hard somehow mean skill despite really kid whole kinda wushu tiredness performance high',\n",
       " 'feel toner getting like calm really spritz soothe little overwhelming thing help',\n",
       " 'member feel hope complete family whether required college neighbor street stranger person better effort situation little realize make school even friend high',\n",
       " 'today going festive ons tree street soon coronation week feeling put even finished catching',\n",
       " 'finding ill seriously',\n",
       " 'grappling horrible man handled unsuccessful alright almost class late resulted stress cry combination week feeling finished',\n",
       " 'wouldnt cause feel anything advance online say prepared stick rule somebody worthwhile face posting rarely always',\n",
       " 'feeling useful',\n",
       " 'pain feel like causing cannot idea passionate imagine becoming group',\n",
       " 'heel feel house dress slightly esteem ratty ever might wear around inclined beautiful pair self ugly jean though sweatshirt definitely bother new buy boost still',\n",
       " 'feeling yet im homesick alright',\n",
       " 'dance feel pretty',\n",
       " 'feel every fabulous morning workout',\n",
       " 'feel word love shall eye vain know touch assuredly return',\n",
       " 'feeling friday started cramp work fabulous morning get wa awful',\n",
       " 'honoured feel call share able friend brotherhood',\n",
       " 'feel stormed black thick rain cloud sky begun town apprehensive',\n",
       " 'fearful feel unsafe reason stated',\n",
       " 'feel side getting like pulled lunch tsa yet quite shaken creative croissant',\n",
       " 'feel getting talkative quiet get awkward really group overcompensate',\n",
       " 'feel finally time lonely around especially excited loved arm companion put never meet disappoint make',\n",
       " 'src feel safe blog img http pretty',\n",
       " 'done feel fun get eager plan pretty quilting',\n",
       " 'ask go feeling horny home let',\n",
       " 'loving feel sense able love think understanding uncomfortable feeling put understand poem pressure',\n",
       " 'feel raising place ancestral stranger home strange father land son',\n",
       " 'feel left gorgeous effortlessly graceful movement confident shower body fantastic wa questioning',\n",
       " 'pram feel happy day long take one around often kid go outside energetic gym walk bouncy run three',\n",
       " 'feeling wa strong still',\n",
       " 'release dumping burst space go rich also tear feeling seemed know devastating thought without',\n",
       " 'latte hot get morning wanted last coffee spice bitter maybe little feeling thing wa pumpkin',\n",
       " 'pain feel last past month suppose questioned bit shes habit exaggeration openly though badly wa hyperchondria never skeptical',\n",
       " 'bed feel assured clench corner',\n",
       " 'selfish feel like amazing grateful also life thing',\n",
       " 'god utterly called seen name affirmed book figured feeling accepted question wa right without',\n",
       " 'rebellious girl short office nice perhaps feeling know wa hair',\n",
       " 'type eid feel miserable celebrates family',\n",
       " 'go car might feeling get really wash need generous',\n",
       " 'night giving feel sleep deprived sleeping u still almost',\n",
       " 'feel really right violent',\n",
       " 'truth content etc lucky bloging dog sitting amp feeling know cat',\n",
       " 'started feeling friday woke funny dog sick',\n",
       " 'feel parent hunt repressed witch memory need therapy protect',\n",
       " 'woman disgusted feel race nationality even associated',\n",
       " 'feeling laughed bitter bitterly wasnt',\n",
       " 'first wished u hated however remember knew could would experience feeling know much forever thought wa',\n",
       " 'feeling look mellow wear beginning cold wa wanted quite easy soft',\n",
       " 'feeling thing ive indecisive think lately maybe need told time',\n",
       " 'disservice artifically feel doe resolved genre story still',\n",
       " 'manitz pissed accent spanish like draw name write artistic line im bigger circle mark feeling know mood dot always',\n",
       " 'hill storm going side terrified remember fall big bus really mountain middle feeling steep brazil wondering wa',\n",
       " 'whimper feel thought uncared could unloved',\n",
       " 'feel going personal youd show ever certainly sit im watch life curious whats tell',\n",
       " 'post im feeling humor sorry funny wasnt',\n",
       " 'world feel ive door got mountain walking foot fantastic running trail fell',\n",
       " 'sweet feel love directly trust one mind girl insecure say person jealous whatever worthy friend tell come',\n",
       " 'strange feel work one back holiday coming day',\n",
       " 'example rushed influenced enhance colon dickinson sonnet clearly dash im use enjambment semi happiness feeling emily instead',\n",
       " 'bread first get looking whether peanut butter girl control yet fatter needy impossible home third life world feeling little thing neglected remain eat hell day',\n",
       " 'genre frustrated redeem leave could head claim beating point either feeling wall entirely',\n",
       " 'feel sad hopeless',\n",
       " 'little feeling im class concept lit intimidated getting',\n",
       " 'feel routine get groggy son still',\n",
       " 'feeling thats going fine popular view isnt',\n",
       " 'feeling forever shocked wonder hold awe want',\n",
       " 'year result might one heard st fear rumour failure exam',\n",
       " 'appreciated feel going love anywhere people valued arent relationship want know even nature change',\n",
       " 'know feel distracted day',\n",
       " 'music feel use afterward always relaxing invigorated regularly',\n",
       " 'feeling im practice eye much distracted possible shut tend',\n",
       " 'see feel ha know perfect even trend youtube influence like w',\n",
       " 'good mak get hard wanted opposite pic tango quite specifically lol feeling away wa far enough shy',\n",
       " 'bed feel weird slept sleeping also two',\n",
       " 'feel today wish earlier made thinking restless needy okay cooped amp thing ha wa',\n",
       " 'unable feel lost fit wear skirt could confident didnt weight tiny ive',\n",
       " 'bit feel creative soon presence blog hope life miss',\n",
       " 'president feel unwelcome current conservative fan made',\n",
       " 'enclose couple verse could thinking feeling much odd weigh curious hear',\n",
       " 'feel complacent life begin',\n",
       " 'alone feel vulnerable',\n",
       " 'feeling thinking example parenting fine wa inspired remember',\n",
       " 'punished thing feel one stupid always chastised behaving getting like',\n",
       " 'feel perfect hope ask stay really isnt would life much right everything',\n",
       " 'im feel simply contented',\n",
       " 'feel important working find project im alone cut everyone comfort meditation life saying solitude',\n",
       " 'feeling im nervous stupid think',\n",
       " 'feel honored',\n",
       " 'feeling god act work amazing wa life',\n",
       " 'bird im feel flying manner carefree air like',\n",
       " 'appropriate feel revise mind person receive reply offended sure make enough sound',\n",
       " 'something away died sad irrevocably mine gone felt friend',\n",
       " 'regretful feel died matt visit would alex coming',\n",
       " 'feel educating supporting educationg mama papa key family',\n",
       " 'toward feel hostile lost like hate im everyone feeling sure',\n",
       " 'feel nothing like boring easy usage say sewing fabric else make since',\n",
       " 'feel scent underwear girly liked lightly glamorous gorgeously use original drawer fragrance always freshen',\n",
       " 'feel work man young talented like',\n",
       " 'thing feel around curious',\n",
       " 'feel happy apart others become im reason life something succeed push help successful',\n",
       " 'feel uncomfortable guess',\n",
       " 'mellow feel may underway process far whatever wounding healing pretty getting',\n",
       " 'feel day u people whether remembrance many wholeheartedly support added armistice wonder reluctant antagonism traditional towards sometimes misunderstanding',\n",
       " 'shitty feel around always leg would mile come like',\n",
       " 'explain feel come best reaction way shaken know right run unfair',\n",
       " 'blessed feel amazing seriously support home',\n",
       " 'feeling feel mostly nervous danger maligned anticipating abused tired',\n",
       " 'restless feeling typing right pretty',\n",
       " 'infrequent touristy feel cawing piling visitor relaxing head place gosman go gentle water lapping breeze wood coming want montauk seagull area know harbor',\n",
       " 'moment exactly feel seriously stressed hilarious start could hy admit card e',\n",
       " 'feeling inarticulate year wrote normal wa last dull',\n",
       " 'ever feel motivation id kick irritable gear',\n",
       " 'child feeling feel innocent illustrating',\n",
       " 'satisfaction younger consumerism driven walking revered looter longer made corpse supporting america feeling contributing ha wa',\n",
       " 'feel resentful start',\n",
       " 'headache like place vacation obligation feeling blackmailed spend three emotional',\n",
       " 'checkup feel wednesday herpes clinic surprised would point coming make',\n",
       " 'feel fucked everyday life like',\n",
       " 'feel year hard like really child class valued ive try worked make',\n",
       " 'feeling disheartened late word',\n",
       " 'feel upon time like retrospect bitchy think trying wa shake',\n",
       " 'truth feel time definitive assured terrible decision guess second making know certain still',\n",
       " 'something egg feel equally delicious benedict like',\n",
       " 'feeling im trusting way',\n",
       " 'feeling favorite im summery element past beach bummin inspired time',\n",
       " 'feel even love gorgeous smiled',\n",
       " 'feel different pang sickness city missing mean discover define amsterdam mindset line american whenever home life blank much continue cultural',\n",
       " 'arcade feel comfortable physically simple practically emotionally purpose absolutely try make',\n",
       " 'say feel sympathetic',\n",
       " 'feeling hopped eye london tube wa ride eager',\n",
       " 'go see one feel online talking another ignored friend unloved',\n",
       " 'feel anything blog monitoring ever dont say please across stop offended come',\n",
       " 'feel one weird cancer time like',\n",
       " 'feel dream joy true well chance maybe safe small want come',\n",
       " 'cranky feeling im',\n",
       " 'administrator feel praise get time ability student think productive puzzling class use also singled particular ha read ssr',\n",
       " 'feel care fill complete people knew tried needed befriending didnt using accepted loser even wa',\n",
       " 'always stressed feel',\n",
       " 'feeling cramp another ignored also remember',\n",
       " 'world mellow feel peace fed slept within external hope well',\n",
       " 'lot mom learned take exhausted guilty positive thinking also better effort tear feeling break',\n",
       " 'feeling baby im new hung awful friend day',\n",
       " 'feel fine relaxed',\n",
       " 'pain feel really suffering',\n",
       " 'go something feel giving sleep precious else life time',\n",
       " 'mom beloved feel blog neglecting like',\n",
       " 'say hw din feel thing keen wrote co xxx shalt',\n",
       " 'im feel edge precipice ahead staring terrified like',\n",
       " 'feel totally uni time free month five gone listless whole front exam come',\n",
       " 'sympathic feel advocate view lobbying politician furious supporting live life die right tell',\n",
       " 'way feel wa abused',\n",
       " 'im feel distracted attention paying still',\n",
       " 'good hot time started hour five block idea examination cold knowing third would feeling tingly ahead still',\n",
       " 'im feel outcome make would hesitant suggestion sincere',\n",
       " 'time terribly feel fond majority precise',\n",
       " 'good feel damaged personal aspect gaining security control',\n",
       " 'tastic feeling bombed freaking interview first wa second left pretty fan',\n",
       " 'good single feel every long u like non taken spice think chance delicious go kind inventory oil specifically recipe something making item buy perishable without sauce build',\n",
       " 'feel dad pick high plant window smelly become jalopy kid child teenager soon whose pull coming school cool advise',\n",
       " 'god feel stuff year traditionalist drag real honest like call victimized think head offending fear skeletal created hesitate ago world remains wa conservative hoax kind country still sand',\n",
       " 'feel kit beautifully lucky call site mine curated material would',\n",
       " 'rubbish blood prescribed tablet iron reason lethargic feeling proved wa low',\n",
       " 'feel life mind dangerous like',\n",
       " 'toward feel others everyone compassionate love least like',\n",
       " 'punished feel neglecting',\n",
       " 'feel doesnt lovely polish capture beauty like',\n",
       " 'le secure feel emotionally socially main financially root lost like',\n",
       " 'feel every fully time like stressed think im decision supported bullet bite',\n",
       " 'awkward tool like come especially sounding immature voice across sharpest knowing tic shed feeling nasal know verbal obnoxiously always kind',\n",
       " 'little feeling id terrible nightmare wa disturbed',\n",
       " 'didnt feel customer passed case disturbed walk want',\n",
       " 'feel able hope find sufferer alone battle want help',\n",
       " 'good exceptionally pretty go timetable tomorrow monday reluctant though feeling even school',\n",
       " 'night feel great festive wheeeeee delicious',\n",
       " 'pain beaten disappointment looking like forward feeling husband emotional',\n",
       " 'crappy feeling im sick',\n",
       " 'feel pattern going convinced almost like',\n",
       " 'antique feel dress like wear underwear maybe bit im slutty obviously also little edgy make wearing',\n",
       " 'reach feel perverse u tempting managed stay pride ordered control self close flesh human wa',\n",
       " 'defiance feel olivias easily impatient blatant outraged prominent characteristic seems annoyed day',\n",
       " 'year morning left last giddy official kid im class positively sad little feeling wa',\n",
       " 'cant feel supportive help also jealous',\n",
       " 'feel written word like clever rather spoken mainly text much',\n",
       " 'seriouly feel around dont respected ordered privacy',\n",
       " 'feel word consider uplifter aspect true devotee hence bandhu said become relieved unworthy benefit person deen title afraid personality hear fallen',\n",
       " 'pain feel suffering personality',\n",
       " 'mother god feel defend finance care died time like month six walt self guess would want quiet make trusting joseph',\n",
       " 'lethargic little feeling starting wa warming',\n",
       " 'game feel nothing keen excelent anymore movie especially really bit im got watch life sometimes anime video since',\n",
       " 'desire attention could please wonder enjoy affection eager master feeling wanting much wrong ha r return',\n",
       " 'quickly first cost hard eaten saving burden become inspired positive bit didnt im weve quite boyfriend want brighter little feeling thing end come',\n",
       " 'mom rather feeling say im feel max lucky right sentimental',\n",
       " 'feel ill crowd show social isolated anti feeling know wrong wa make',\n",
       " 'mother turn continuously learned find often suicidalness completely cry blank feeling ive emotion',\n",
       " 'feel style loyal',\n",
       " 'vulnerable feel london youd precise rather bit way may understand homeless',\n",
       " 'waste feeling listless space finally stop like',\n",
       " 'feeling im know agitated side dose effect high',\n",
       " 'positive feel shift',\n",
       " 'feeling brave enough',\n",
       " 'thief fear',\n",
       " 'nov clever feel',\n",
       " 'mean spend feel exit always dissatisfied money store',\n",
       " 'feeling im really quite angry',\n",
       " 'didnt civil union feel supporting kerry gay equality',\n",
       " 'really ashamed feel',\n",
       " 'feel amazing life people',\n",
       " 'feeling intelligent judged left finally ridiculed',\n",
       " 'bit feel insulted starting stranger',\n",
       " 'feel today road many brick hopeless shining light little brighter yellow wa end day',\n",
       " 'sorrowful actually feel',\n",
       " 'woman see curse feel envious boot wearing want',\n",
       " 'bit loving feel keep together apologize make tell',\n",
       " 'go say feeling impressed klein right youre naomi',\n",
       " 'sens feel came cold black began unconsciousness dull',\n",
       " 'suspect feel fond private le',\n",
       " 'faux comfortable hawk feel young going talking back felt ask honoured dykey must safe looked woman thing wa enough kind',\n",
       " 'feel perfect part complimented cast wonder caveman insulted told',\n",
       " 'feel lighted plot tag many plus relatively everyone fund diverse economically sure price convinced come',\n",
       " 'baby feel empty isnt',\n",
       " 'href feeling exhausted stopped http provokingbeauty',\n",
       " 'feeling im friendly font',\n",
       " 'feel working calm breathing list afternoon accomplish hand bead consciously thing',\n",
       " 'ever feel especially young wa always compare intimidated since people',\n",
       " 'feel rotten made',\n",
       " 'started feeling hemorrhoid hostile checking',\n",
       " 'feel get shoe love trimming pair glamourous new designer box wearing come',\n",
       " 'largely romantic successfully sexual feeling know fact hidden',\n",
       " 'something feeling someone precious warmly love hugging small',\n",
       " 'feeling im mellow normal far',\n",
       " 'outfit people like started dismayed studied way though became feeling bought gorgeous wearing',\n",
       " 'toward anything spare wax get brave need draw rid unibrow dont maybe creeping im want feeling eyelid hair',\n",
       " 'feeling im px outside li margin list width style border lucky',\n",
       " 'st feeling feel recall olympic watched school longing runner high',\n",
       " 'feeling cue gather coaching tip bodypump compulsory watched woke dvd confident',\n",
       " 'say didnt feeling first wa make always skeptical strong',\n",
       " 'limited phone feel current talk time arrangment bring want hesitant',\n",
       " 'good might feel beginning pas chance',\n",
       " 'sweet feel like pea control better little know ahead help',\n",
       " 'feel attack receiving end violent like',\n",
       " 'file feel familiar batch document worthwhile people',\n",
       " 'feeling many especially thankful right blessing life small',\n",
       " 'dont stay tomorrow hoping home weatherman feeling forecast venture creative rain right kind',\n",
       " 'moment feel microcosm awkward missed like one im outside id edge party',\n",
       " 'bit feel funny actually',\n",
       " 'feel nothing year last learnt best trusting two way else instinct',\n",
       " 'feeling blossoming eager also anxiety',\n",
       " 'share feel burdened',\n",
       " 'feel get side doesnt weird often sleeping nemo though want happen even always without',\n",
       " 'feeling heel im outfit gorgeous',\n",
       " 'feel confused',\n",
       " 'forward session feel wa useful tool gave need move life',\n",
       " 'selfish feel bringing lost grieving child baby parent know newtown loneliness',\n",
       " 'lose lord disappointed love loyalty heal betrayed one loyal took away feeling heart paining never even gave',\n",
       " 'embarrassed feel envious',\n",
       " 'frantic feel make could need',\n",
       " 'someone honor precious teacher gift responsibility overwhelmed feeling trusting',\n",
       " 'feel right tranquil great',\n",
       " 'music feel lyric nothing new frustrated clearly',\n",
       " 'work going wanted talk decided find broke unloved feeling thing try thought shocked wa steve',\n",
       " 'wouldnt incredibly making feel unwelcome bouncer spoken completely witness emo beared kid well',\n",
       " 'feeling stressed breathing tend stop',\n",
       " 'feel year quit terrified knew free cant book believe life ago week smoker wa right finished without cigarette',\n",
       " 'limited grocery total willingness complete important talk etc risk sensation clerk step matter go experience small mailbox mailman feeling thought whatever emerge',\n",
       " 'frantic feeling already',\n",
       " 'good started feel thing first pictured writing insecurity like',\n",
       " 'feel insulted complimented',\n",
       " 'fall crave submission feel submissive least',\n",
       " 'tender feel fine',\n",
       " 'someone moment feel pissed deep wake like irritated slumber edge even falling',\n",
       " 'little feeling work groggy made',\n",
       " 'thats like loving feel sort love keeping hindrance want',\n",
       " 'feel fond really ok second sensitive spelled thought everything',\n",
       " 'feeling quarter round im one ill get hopeful',\n",
       " 'already im unwelcome feel starting tell life people',\n",
       " 'baby feel doe fantastic hope absolutely',\n",
       " 'accumulate snow fall tree sleeve shirt watched feeling shoveling vigorous conifer wa',\n",
       " 'feel bitter honkerblonked general',\n",
       " 'feeling already father convinced',\n",
       " 'feel nothing dream love like gotten passionate taste stop living',\n",
       " 'outdoors toe glee anything mud squeal pretty play take place enjoy feeling much rain squishing',\n",
       " 'feeling humorous put caller hold cold',\n",
       " 'feel blog thread miserable busy never make stuff',\n",
       " 'someone god feel finding back give potential energized life something society heart curious',\n",
       " 'lot feel work privileged also',\n",
       " 'little im happy feel nauseated side thrilled pretty',\n",
       " 'hurt feel pitiful eye wailing close taste cry anger tear saturating heart salty sound hear',\n",
       " 'total hold substance island free virtually lettuce sugar leaf thousand large fat feeling bought becomes impressed quantity burn cucumber friend',\n",
       " 'feel swing virginia upon agree barrage handful im folk state constant put sure insulted florida',\n",
       " 'im feel man death innocent row putting like',\n",
       " 'smart feel sure help else',\n",
       " 'moment feel waking work every begin devoted',\n",
       " 'lesson feel thankful learning',\n",
       " 'child see feel sadness longing family',\n",
       " 'naughty little distinctly clermont feel one seem focus called',\n",
       " 'thing feel glad many cherish memory hope gave including',\n",
       " 'feeling right worthless pretty',\n",
       " 'feel first wrote useful project one clever instructables figured would weekend sharing enough start',\n",
       " 'done think abused line wed probably got completely would anger along feeling fight continued dish',\n",
       " 'feel paris u especially student grade unsure st didnt finland suominen would janne little thought ahead nervous wa landed',\n",
       " 'moment feel envious certain kid',\n",
       " 'crappy mom feeling wa pretty like',\n",
       " 'week feeling im aac kind miserable pissed frustrated life whole',\n",
       " 'im feel scared fine surgery terrific anymore',\n",
       " 'feel around vulnerable still',\n",
       " 'feel sure stunned honestly',\n",
       " 'feel attract caring person people life',\n",
       " 'embarrassed feeling say im nothing new quite left',\n",
       " 'dead feel anything get scared worse place buti cant unwelcome ok staying situation thing afraid end decide',\n",
       " 'child thanking may making feel special cried loved',\n",
       " 'feel play art amazing really go gallery yardage want buy',\n",
       " 'loyal self shes feel losing sense adapt think like',\n",
       " 'friday feel one several big loved havent since miss seen kid burdened',\n",
       " 'feel really abdomen low still',\n",
       " 'ever tight alone feel welcomed sure knit make family like',\n",
       " 'doubtful feel',\n",
       " 'feel year however student lego motor use smaller fine want usually delay successful',\n",
       " 'something im feel get meantime exercise worthwhile like',\n",
       " 'feel pity people like misplaced fulfilling rich live life way look know',\n",
       " 'blessed feel amazed yes excited',\n",
       " 'feel talking hesitant',\n",
       " 'minute skip get least five known though week lethargic feeling particularly try workout lazy day',\n",
       " 'ever feeling devoid beaten utterly hope sick life want',\n",
       " 'pileon feel interested learning learned project applying past add seems clear experience want comment hesitant involved future',\n",
       " 'feel personally passionate quite tremendous importance communion theologically',\n",
       " 'feeling im happy well',\n",
       " 'trapped able depressed city time like missing find write think constant imagine something feeling much near without actually',\n",
       " 'brig others desire strengthening financially become sincere christianity got strong relationship also feeling ha building help successful',\n",
       " 'feeling feel ive liked would often lot fake like',\n",
       " 'feeling temple bike going rest energetic decided',\n",
       " 'feel dream ight excess blamed toni seen like seldom said really er merican im hah way witnessed understand never',\n",
       " 'good athletics result field academic get happpy',\n",
       " 'feel work anxious sort stressed dont rule want require something know job may make always whatever pressure',\n",
       " 'something feeling chair id pop worthwhile like',\n",
       " 'feel get u like really could control delighted punching onto hoping second headed trail wood ahead navigating wa far enough theyd',\n",
       " 'size feel like think angry could beautiful thats would never normally actually',\n",
       " 'crappy turn feel work people like result many perspective think wonder wanting without sake',\n",
       " 'whether feel book finn huckleberry censored indecisive',\n",
       " 'feel nozomi like stomach must think known knot fool unpleasant kun laugh kamiki nonchan',\n",
       " 'level old recognizing choose kick shut made reaction choice feeling keep persistence allowing start unloved',\n",
       " 'cant say im feel wait celebrated made day hated till like',\n",
       " 'feel cheerleader funny writing guide like call really maybe observant something spirit still',\n",
       " 'feel nothing find depressing surprised rather would grey life unrewarding',\n",
       " 'feel comfortable decision',\n",
       " 'stomach episode feel free sweaty get really funny like falling',\n",
       " 'feeling lately overwhelmed',\n",
       " 'mother brand docrat feel latest tanning asma lash learnt fit perfectly stolen lovely boudoir wonderful world much beaubronz mantra',\n",
       " 'good feel get writing pretty like rebellious watching romcoms feeling may particularly',\n",
       " 'didnt quickly feeling feel welcomed happy first entered changed morris left',\n",
       " 'feeling im contemplating skeptical',\n",
       " 'different anxious people time regard certainly siege said recent become disadvantaged often faith le considered amongst belief looked way feeling ive make worthy',\n",
       " 'bit im feel heartless care like',\n",
       " 'single feel every horrible cell chemo hate toxin making thought washing make cringe',\n",
       " 'honoured feel hosted talented series',\n",
       " 'little threatened feel around love',\n",
       " 'feel throat blog upset lump brought eye shouldnt write allthingsbucks lame worthwhile tear feeling thing reading',\n",
       " 'naughty feeling didnt take long wa start nomming stuff determined',\n",
       " 'feel many work see artist honored public display wonderful',\n",
       " 'feel lively fully remember stressed fun could',\n",
       " 'href feel twitter http dazed',\n",
       " 'feel bitchy hurting',\n",
       " 'pic feel ive always assaulted like',\n",
       " 'wrap working pretty people lucky seems im also life week feeling job new thankful everything',\n",
       " 'isolated incredibly lonely feel',\n",
       " 'someone selfish feel thyroid hard anyone talk yes really think whiny dramatic terrible fine want else look understand still',\n",
       " 'inside spent year eating blood time needle non fond nineteen old head im yesterday meat perhaps feeling dizzy particularly wa right test doctor female',\n",
       " 'someone work need pretty unless isolated admit worried feeling much cubicle wa',\n",
       " 'feeling im sweet fairly life like',\n",
       " 'someone feel insult going clean love unilaterally carelessness time abuse thereby forgiving wronged forget intentionally slate perceived',\n",
       " 'feel year get love like find kid harder passionate want job teach try even',\n",
       " 'pain sweet smile people one im second anger feeling bury giving even',\n",
       " 'feel work reassuring massage like knew spirit plus gooooood way holy wa divine',\n",
       " 'career im feel ive respected reached highly point though',\n",
       " 'light feel one sometimes awkward make really group conversation small',\n",
       " 'good energy feel lately clumsy lot gigantic bad slow also ive fabulous day',\n",
       " 'like know feel google seo hated also',\n",
       " 'didnt wink night feeling sleep continued get fabulous next morning',\n",
       " 'mistake time take feel sleep fucked like',\n",
       " 'love man sliding big body doe throbbing nice cock feeling gorgeous',\n",
       " 'feeling bit stock uncertain market novel found position rally',\n",
       " 'feel like girl needy seem want judging know',\n",
       " 'festive feeling im definitely',\n",
       " 'feel presence burdened',\n",
       " 'disbelieving little feeling god feel sort dazed oh still',\n",
       " 'date closer getting excited due seem week feeling super flying',\n",
       " 'feel year back carefree getting like irresponcible college turned way page',\n",
       " 'limit favourite u dvd like im book choice generous something feeling uk',\n",
       " 'feeling time happier long alive energetic',\n",
       " 'feel confronted really pathetic',\n",
       " 'game feel activity dangerous',\n",
       " 'world feel class fantastic would benzema player mould',\n",
       " 'prize feeling trying terribly keep eye impending anxiety burdened',\n",
       " 'turn feel love layered well could unpleasant healing expectation forgiveness thing',\n",
       " 'feeling supposed im completed nothing many thing gloomy complete though',\n",
       " 'feel working like amount would ha far successful change lifestyle',\n",
       " 'feel buck love working big satisfied really bring say industry porn fulfilled',\n",
       " 'overhear geek tune feel ringtone triumphant victory',\n",
       " 'child literature author feel thing love dumb need kid',\n",
       " 'soo mixture well big isolated bit sleeping feeling quiet wa group',\n",
       " 'greedy feel hungry money absolutely',\n",
       " 'feel fucked shut want',\n",
       " 'feel love solely relying fact topic actual knowledge opinion passionate certain instead position backing',\n",
       " 'feel today like book suffering home way everytime exam open went day',\n",
       " 'feel make surprised',\n",
       " 'feeling morning woke jubilant hike',\n",
       " 'game feel like mom kid touched ball proud whose soccer little',\n",
       " 'inside outside feel miserable like',\n",
       " 'back limitation find held must resentful way older feeling without accept',\n",
       " 'incredibly feel people amazing charmed life exciting thing chapter',\n",
       " 'feel u need people however time responsible action around judge found life also wronged make actually',\n",
       " 'belongingness seek part stem mainstream feeling know accepted community importantly',\n",
       " 'feel special doe song really edition like',\n",
       " 'feel course blog mini must thia sandwich loyal baker fan constantly scruncher rex',\n",
       " 'mother feeling im tongue currently fucked paper way',\n",
       " 'feel innocent love',\n",
       " 'breakfast sweet feel like dessert often yet glycemic meal low carby',\n",
       " 'feel languagedirection ovation en isprivateblog could standing safe false type feedlinks alternate use xml atom link utf title locale ismobile mobileclass encoding application ltr isprivate rel',\n",
       " 'notice lose feel get homemaking lot worse halloween around desire really month six longer reprieve im youll mark though instinct creative pregnant nesting post',\n",
       " 'god feel uncertainty desire past despite doubt self way gracious opportunity giving moz return heat',\n",
       " 'feel felt resource lucky really needed asking found always answer',\n",
       " 'hill feel hurt tire calf fitness foot fine running also along keep wise starting begin',\n",
       " 'feel sense every drag assaulted like darkness sensitive away always',\n",
       " 'adding supposed feel edge thankful sarcastic ought age',\n",
       " 'feel couldnt totally utterly enemy band almost helpless kidnapped distraught death tortured desperate lorena freedom wa help',\n",
       " 'feel unfortunate cheer long well period mister magnum race travelling wa sound',\n",
       " 'tragic feel sparkle tortured enough without importance',\n",
       " 'thinking lonely selfish time feel way',\n",
       " 'energy feel drained',\n",
       " 'mom feeling horny day masturbate chatted think also amp',\n",
       " 'feeling shitty horrible pretty stinkin reviewer',\n",
       " 'helpless feel',\n",
       " 'feeling wa awful sunday',\n",
       " 'silent falsely feel sense largely ever started dont weird expressing said inflated articulate opinion know importance pressure audience',\n",
       " 'nap grumpy feel agitated extremely woke',\n",
       " 'feel hard word missed lot pretty like studying exact mind really portray crossed discovering meaning ive never',\n",
       " 'lonely bird im feel work wa school social usually',\n",
       " 'feel one free comment love post',\n",
       " 'woman feel tree intimidated great family',\n",
       " 'feel worker lot family truly kid positive condition thing help',\n",
       " 'feel back conflict fly oblivious think plague helpless reason would want resolving may uae',\n",
       " 'didnt saw feel kissing surprisingly cena maria video hated like',\n",
       " 'rather feel unkind like',\n",
       " 'feeling wa submissive',\n",
       " 'bit feel weather obnoxious definately',\n",
       " 'feel bringing say torso siwons close pressing face breath agitated enough',\n",
       " 'moment bloom word ever one treasured indifferent want feeling keep since tell start',\n",
       " 'entire feel casual future',\n",
       " 'feeling lift couple missed wa strong still',\n",
       " 'hour feeling several sat low couch pretty',\n",
       " 'happy long back time checked around say everyone way ago contented feeling wa everything',\n",
       " 'sweet snuggle happened growing time like fun milestone big write baby im something feeling ha trip personality tell',\n",
       " 'feel man valued worthwhile must hate need expectation life',\n",
       " 'loving feel conscious u kindness various mind think body allow thing interact',\n",
       " 'blessed feel completely group part',\n",
       " 'skin feel product oily moisturizer doesnt designed type im lovely contain use oil moisturizing ton ingredient make without',\n",
       " 'im cause feel environment afraid make relaxed',\n",
       " 'physically spider shirt would overwhelmed way want feeling wa shake everything',\n",
       " 'size feel discussion height georgia family serif color line say race font watch style px piece little agitated',\n",
       " 'wont feel deny crush back love find broke dont said cant bit home heart much touch wa come',\n",
       " 'feel frustrated like around unwelcome weariness shoulder slipped life little heart ha settled friend ache longing',\n",
       " 'crappy feeling trying nowhere year closing even wa near last remember',\n",
       " 'house going well broke accidentally player cut uncle feared radio playing wa',\n",
       " 'follower pagan conflicted ever whether originated modern celebrate popular christ holiday feeling day',\n",
       " 'feeling pill wa impatient took',\n",
       " 'yet weightless feel carefree grounded worried time',\n",
       " 'opposition civil feel past repentant right sincere',\n",
       " 'feel like mom one daycare made parent terrible point constantly put son',\n",
       " 'super relax thing feel guilty list',\n",
       " 'feel motif process inbalance reconciliation uncertain counselling committment',\n",
       " 'feel valuable really like',\n",
       " 'go feel emotion angry like',\n",
       " 'feel written assaulted grammar like training im language something much ive kind reading',\n",
       " 'feeling insulted everyone sneha comparing',\n",
       " 'unable every morning able wake family lucky blessed call amazing mine im believe life feeling enough still',\n",
       " 'god feel love hope leader people like one depict supportive george way thought wa country w',\n",
       " 'work hands like bed one fulfill desire aahhh really afternoon magic horny sexual forgetting feeling creative thought wa help',\n",
       " 'deprived food must frugal live determined feeling know scone without lifestyle',\n",
       " 'giggle nervously threatened feel',\n",
       " 'inside thing feel horrible kill make better like helplessness',\n",
       " 'feel house ill messy clean photo online bit picture taking far post',\n",
       " 'loathed hated feel',\n",
       " 'little feeling picked apprehensive',\n",
       " 'feeling numb ive',\n",
       " 'bit feeling im bitch every vindictive spiteful time',\n",
       " 'feeling look eye got wa sincere',\n",
       " 'feel one assured mind',\n",
       " 'feel unfortunate dont soundtrack',\n",
       " 'feel nothing reassures believe father nervous make',\n",
       " 'feel dream date complete around wedding uptight mine pursue min nervs little thing right always day',\n",
       " 'feel today cranky think suppose would fast come',\n",
       " 'feeling im going stubborn',\n",
       " 'lonely feel lost',\n",
       " 'empty feel would',\n",
       " 'feel agitated cranky right patience lacking start',\n",
       " 'mad feel one create explanation easy le im twist little know thing sure help still',\n",
       " 'feeling blessed yup know rich',\n",
       " 'didnt feel afraid tell want',\n",
       " 'coverage feel ugly put sometime love medium picture',\n",
       " 'little neglectful feel fellow blogger though',\n",
       " 'frame feel going sort upset eye hate glass le paranoid im bigger hoping huge quite small look wa make since',\n",
       " 'cant moment dish im feel washing believe useful',\n",
       " 'music feel distressed mind fma rewrite op',\n",
       " 'feeling lonely im half monday quite term',\n",
       " 'feel get awkward adventurous people like socially dont really thing new meet group',\n",
       " 'feel shape admire people like severely entertained athleticism got participate watch would event olympic',\n",
       " 'gotten feel horrible workout havent worse made fact',\n",
       " 'bit secretary feeling would irritable eh imagine hate day',\n",
       " 'im feel unhappy',\n",
       " 'take flower feel care hive wonderfully loyal time like',\n",
       " 'amazed feeling remember',\n",
       " 'sharper pleasant see thick encased painful place shell would moss feeling know heart never muffling layer break',\n",
       " 'mid indentured feel awash attached underpaid bottom find peer modern marginally chemistry sea debt duped better life graduate promise law school overeducated nameless',\n",
       " 'feel contribute blog regard truly especially writer world educating valuable',\n",
       " 'ton time feel thing get clever would like',\n",
       " 'daily feel happy patient awake breast milk baby im shes bad believe formula running energetic better little know much right still',\n",
       " 'quickly feel trotted ask embarrassed added hoping conversation would enter kind',\n",
       " 'feeling spent got drunk next sleeping groggy wa hated whole day',\n",
       " 'feel discussion love faking write feminist go class passionate though issue',\n",
       " 'welcome feeling threatened baby home mommyhood breastfeed inability would end want',\n",
       " 'result feel peaceful calm agitated pleasant opposite',\n",
       " 'feel class student honored come',\n",
       " 'good feel hour especially attention often sex horny want tell',\n",
       " 'itll feeling already thing feel anxious need flawless get xox soon real',\n",
       " 'blessed feel rich joyful',\n",
       " 'hugely feel dream course every last horrible online really unsolved matter made paranoid crime night didnt theory week help anxiety reading conspiracy',\n",
       " 'mad feel truly get enough hate somehow',\n",
       " 'feeling became uncertain especially teenager young wa remember',\n",
       " 'feel get long feed cant wither would away melancholy die far enough',\n",
       " 'offended defiance enter different feel allowed coming culture want',\n",
       " 'feeling ate rebellious wa',\n",
       " 'restless feel edge agitated quite often',\n",
       " 'using feel idea awesome word actually uncomfortable',\n",
       " 'rather pleased feeling im tonight',\n",
       " 'gentle lead feel kind like',\n",
       " 'single rather feel every borne second actively hated like',\n",
       " 'feeling im scare indecisive',\n",
       " 'socket tug feel eye violent',\n",
       " 'nowwwwww feel carefree',\n",
       " 'arab threatened feel yearning dancing deep fully normality emotional armed believe appears peace displayed wa soldier semblance since',\n",
       " 'feeling discouraged wa left hopeless',\n",
       " 'feel fixed going get road messy back',\n",
       " 'trial feel hard long frustrated error easier garden learning remember give patience success better far blessing research run day',\n",
       " 'able photo sick well month go whitleys patience would cold shoot amp feeling wondering wa even',\n",
       " 'little feel saw bitter wa tempted',\n",
       " 'feel anyways back kind doha insecure',\n",
       " 'feel upset frantic around longer grateful fix emotional urge',\n",
       " 'strange feel brew',\n",
       " 'feel people well unrelated student compared faired strongly due completely quite punished badly group',\n",
       " 'like job feel valued permanent want',\n",
       " 'someone feel exert love care important like progress emotional spiritual point intellectual',\n",
       " 'feel comfortable others happy japanese current progress pretty speaking actually',\n",
       " 'pain different feel family real like learned one pass plan arrangement loved thing even',\n",
       " 'feel dating vain least would',\n",
       " 'feel written important like form baby record keeping life picture even',\n",
       " 'feel miserable gloomy glum heartbroken forelorn unhappy wretched',\n",
       " 'little feeling bit really irritable depressed horribly',\n",
       " 'feel join series honoured lady part womanhood celebration wonderful talented',\n",
       " 'hour feel team right four ago confident',\n",
       " 'car feeling restless house simply sorry got said',\n",
       " 'bit feeling im ive see everyone really noticable like paranoid',\n",
       " 'feel birding everyone fabulous weekend',\n",
       " 'little feel baachan bus trolley squeak stop hear paranoid',\n",
       " 'feeling im thing certain planned summer rushed longer like',\n",
       " 'feel breeze cold',\n",
       " 'feel try desire calm breathe frustrated',\n",
       " 'feel pretty suggestion warning language improve open benign',\n",
       " 'country feel danielle knew broke really go didnt trying losing headed would military protect thing prepare heart heartbreak husband wa even wife thru',\n",
       " 'night talented feel suddenly smarter lot last',\n",
       " 'feel doe everyone buy get amazing smell actually like',\n",
       " 'feel motion quilting delighted bear cant idea ago though even day',\n",
       " 'feeling feel believe love bored heartless anymore',\n",
       " 'vulnerable making feel even shoeless slowing ended',\n",
       " 'god discussion dennis must noted dangerous article say book closing critical admit something feeling ha merit chapter highly business covingtons',\n",
       " 'course rufus kate somehow retrospect loudons ironically wainwright refers cant rather mcgarrigle though feeling son talented help',\n",
       " 'lonely feel birthday day',\n",
       " 'look sweet feel captured like',\n",
       " 'feeling envisioned im whether got intended right intention vision unsure',\n",
       " 'feel share cute need picture like',\n",
       " 'feeling wa low',\n",
       " 'god feel sometimes really love angry enough havent',\n",
       " 'feel rafael sorry bosch',\n",
       " 'feel missed back attend important hope people life blogging thing certain still come',\n",
       " 'feel comfortable house time almost im staying bored relentless tired something asked without money',\n",
       " 'moment today decided print popular offer im guess generous feeling fabulous deal',\n",
       " 'fandom feel unpleasant anime general begin',\n",
       " 'tender feel disoriented',\n",
       " 'feeling today carefree really wa',\n",
       " 'energy feel sleep lot sick hope getting med miserable amount zero havent tomorrow hoping cold better choice feeling much vitamin fluid high',\n",
       " 'feeling im horrible cranky',\n",
       " 'single wont comfortable feel one amount im slutty person would relationship',\n",
       " 'feel front ongoing door relocation troubled',\n",
       " 'feel healthy family free said live making fabulous wa asked country',\n",
       " 'energy incredibly today content optimistic complete however terrible cold amp feeling lack woke',\n",
       " 'feeling im relieved terrified combination',\n",
       " 'feel really wa patient wronged',\n",
       " 'talented feel many gulam even singer classical ali',\n",
       " 'someone feel time special guy go genuine would want spend right make treat quality',\n",
       " 'seeing feel love need people gear think favorite share passionate hoping live workout without comment',\n",
       " 'strange started feel sale',\n",
       " 'notice feel according rest greatly distressed hiding though world keenly fabulous even always',\n",
       " 'mother feel offspring guilty health problem shielding better shaken',\n",
       " 'little take greedy feel hey among snotface traitor looking like cover',\n",
       " 'hookah feel rude wa hole like',\n",
       " 'beloved feel reciprocated',\n",
       " 'feel bryan heartbroken',\n",
       " 'feel everything fake like',\n",
       " 'differently leader feel believed perfectly truthful would',\n",
       " 'skin feel tree droppings cant outside sticky foot rain help longing',\n",
       " 'feel behavior like violent control made fault believed husband wa even existed',\n",
       " 'feel important time like endeavoring learned one honesty tough lesson youve along authenticity awesome build reveal audience',\n",
       " 'gwiyomi disgusted feel act action cute need like',\n",
       " 'incredibly feel said thankful whole',\n",
       " 'feel press shine eager inspired sun',\n",
       " 'feeling im friendly today hoping thigh walk',\n",
       " 'feeling position else sure nervous wa would anyone like',\n",
       " 'might alone feel frightened left wa spending time older',\n",
       " 'say something feel accepted poisoned must chalice',\n",
       " 'room nothing talking get preferably galley lot need people time dark mean whale noise crew leave place im alone relatively introvert drained energized small feeling quiet',\n",
       " 'feeling im ill give story generous well',\n",
       " 'sweet feel paris scent pretty time like decided find bath together bodywork didnt generic predictable throw collection invest two created amp much actually',\n",
       " 'feel carefree time like dont become recognize liberated person much',\n",
       " 'feel anything happy anymore numb dont angry sad feeling know thought wa right',\n",
       " 'discontent feel day life',\n",
       " 'happens god feel love dealing like torture w plan bit im reason weve tortured would way whole woman c b ha buy starting fertility issue everything',\n",
       " 'feel truly taking long impatient',\n",
       " 'say feel bitter angry come want',\n",
       " 'face know feel customer irate like',\n",
       " 'feel people gone moronic completely life way put trip make always worthy enough',\n",
       " 'fine feeling im',\n",
       " 'prep night feeling feel another sleepless ashamed help way',\n",
       " 'week pleased feeling im quite',\n",
       " 'area feel destruction shaky damage complete worst hit still',\n",
       " 'feel win oscar missed ocean throw would something career',\n",
       " 'inside feel nov terror prince civilian privileged war blackwater erik monday autobiography story th released hero warrior narrated unsung',\n",
       " 'neediness least recently wasnt im needy feeling dependency prior used',\n",
       " 'forward feeling looking hopeful',\n",
       " 'supposed feel funny humanize consistently really place yet balancing mccarthy character fact career',\n",
       " 'uncertain feel uneasy',\n",
       " 'feel around comfortable',\n",
       " 'opportunity feel privileged part',\n",
       " 'transition feel going nervous back expect knowing america like',\n",
       " 'started feel skeptical kind',\n",
       " 'little feel parent sometimes boy inadequate love',\n",
       " 'usual perverse applying predominantly arranged function create month previous style feeling thing much keep fix ingredient right twelve',\n",
       " 'feel work today going like stressed havent im bad life week ive worked',\n",
       " 'feel sad discouraged',\n",
       " 'mother feel guest perfect well autistic boy blogger recognized passionate would thought respected message',\n",
       " 'mellow back depressed cycle beginning really happiness feeling ive coursing',\n",
       " 'moment star amazed via pool drinking nagalene spritzer connecting im california sitting wine wifi figure feeling using google currently stellarium ness',\n",
       " 'feel back washington yes whether least go baby probably completely want know permanently torn convinced bleu although still',\n",
       " 'feel different sense similarly move people guilt situated arent also neighborhood strange',\n",
       " 'possibly red deck heavy see one color casual blue popular could tournament step place rotates block favorite table feeling otherwise hell zendikar',\n",
       " 'pour feeling im agitated brandy coffee',\n",
       " 'feel considering way people pretending sincere',\n",
       " 'good certainly feel read cover detail even give period sharpe enough',\n",
       " 'feel personal like plateau reached satisfied im use style wardrobe feeling much buying',\n",
       " 'feel hard talk people like learned hour friendly month way stop three',\n",
       " 'degree feel away amazed biology wa blown like',\n",
       " 'gentle bother feel doesnt vibration hear',\n",
       " 'feel unhappy incomplete',\n",
       " 'pretty name special girl connection lovely use middle poppy feeling ha certain',\n",
       " 'something feel happen going wonderful',\n",
       " 'feel rejected unwanted',\n",
       " 'feel going kill getting shot loud didnt beast im mark want die accepted screamed thought since',\n",
       " 'energy feel ill get let important broaden debate light little world try sometimes engage view ignorance',\n",
       " 'lousy dreary cheerful im feeling becomes may tolerable dismiss bright gorgeous day',\n",
       " 'find way feel moral every going sure pretty make rich damn obligation assume day',\n",
       " 'pleasant cover done sweetly really say bad would something feeling thing thought wa whatever',\n",
       " 'something feel anything dont special ironic whenever broadly tell smile still',\n",
       " 'good feel couldnt like really brad moral eric nice picture ha innocent belong respectful still',\n",
       " 'feeling ha sense wat unsure let',\n",
       " 'moment feeling future apprehensive',\n",
       " 'good someone feel course favour extent reluctant extremely friend',\n",
       " 'someone room house felt see noise think could night sleeping middle moving wa woke shadow hear',\n",
       " 'time come feel ha sometimes life resentful',\n",
       " 'moment feeling towards around everyone amorous began stage experienced people',\n",
       " 'feel disheartened sometimes realise far culture',\n",
       " 'happy last time pray blessed say im fear overwhelmed birthday feeling',\n",
       " 'little exactly feel keep need popular pressure also',\n",
       " 'feel control hopeless',\n",
       " 'feel hour one horrible best babysitting cut got trip afford wa friend stressing cancel guy',\n",
       " 'came feel climbed people made child unwelcome mountain frightened everyone across village ran looked way away wa asked make stared',\n",
       " 'feel work dont problem inhibited',\n",
       " 'might kept feel benevolent chance hope cry took',\n",
       " 'good god feel looking hope people time done solace horrible proof maybe rather something inspiring',\n",
       " 'feeling im melancholy today back school stuff',\n",
       " 'fearful stomach heart feel clench race begin doubt',\n",
       " 'feel superior wealth guilt enjoys ro liked intimidated anna',\n",
       " 'feel cranky really school like day',\n",
       " 'feel hen coo red belly like soft block interest baby watch firstborn gather thrilled doll little look tuck cuddle help',\n",
       " 'feel perceives happened real like possible stay mountain faithful event try',\n",
       " 'feel ban suspicious walia tarun',\n",
       " 'toward feel others hostile fail nice like',\n",
       " 'path little see glad feel share blog ahead danger',\n",
       " 'feeling cranky sort wake',\n",
       " 'say im feel tortured like',\n",
       " 'year back able looking time old wish delighted part cant rather child book thousand also living jealous nine feeling world read wa help',\n",
       " 'bit feeling im today cranky',\n",
       " 'feel get movie dare biggest could vaughn say stiller hater character performance annoyed enjoyment even still',\n",
       " 'feel protein lobia spice one punjabi warmer masala rich recipe winter punch mostly make',\n",
       " 'feel truly terrifying shaken uneasy still',\n",
       " 'thing feel long empty begin amp burdened',\n",
       " 'good daily feel back learning old write mind memory space race quite nostalgic park used day',\n",
       " 'feel numb f',\n",
       " 'little feeling fearful tense tonight sit pensive',\n",
       " 'someone feeling know thought keep faithful loyal want reader',\n",
       " 'anticipation feel joy right divine',\n",
       " 'feel house behind dont loaded theyre ready overwhelmed feeling keep wa asked bqff',\n",
       " 'investigate mythology feel around personal able subject thrilled',\n",
       " 'feel creative happy right make',\n",
       " 'trying feel wear trip fabulous even without casual could shopping',\n",
       " 'feel happy back like mean others step yet doe allow want even deserve',\n",
       " 'physical feel sort horrible greater start limiting causing reasonably exercise injury without',\n",
       " 'feel year amazed old always subject would impacted',\n",
       " 'portland feel land live food delicious lucky',\n",
       " 'stopping feeling feel cranky morning wa really lot better pretty made',\n",
       " 'didnt night feel every fire cold',\n",
       " 'observing cranky moon mention gravity zero brother im also scientist character feeling main',\n",
       " 'feeling im sociable folk',\n",
       " 'feel excitement agnostic force strongly atheist wonder fundamentalist christian feeling heart judeo sometimes divine',\n",
       " 'started feel one thought day discouraged',\n",
       " 'loving feel going true like maybe im terrible stop wife',\n",
       " 'already outfit time wee newborn outgrown fave im sadly feeling ha thrifted wa sentimental',\n",
       " 'stand night feel ive one version innocent like',\n",
       " 'feel annoyed sad ok typing finish realise angry even longer post',\n",
       " 'feel anything snobbish rate take write wish refuse say doe cannot book winterson explore essentially scope publisher jeanette',\n",
       " 'feel fluttering fleshed real like arent petty though thought even kind',\n",
       " 'feeling uncertain vertigo sad angry left felt apprehensive',\n",
       " 'feel ive handle got instinct trusting like',\n",
       " 'smart thinking feel complacent start',\n",
       " 'already feel back coz uptight admitted half defeat lake asked come',\n",
       " 'feel blamed journalist',\n",
       " 'feel amazed pas missed able pretty bed really month class semester ridden much wa combined',\n",
       " 'arm feel around put safe leg wrap',\n",
       " 'feel lucy loyal',\n",
       " 'im feel boring enough obnoxious personality pretty like',\n",
       " 'feel relieved excited confident',\n",
       " 'crafted feel care people click im proud wonderful something even make enough',\n",
       " 'feel song need one beginning switched little melancholy wa make fuck',\n",
       " 'moment feel talk time like embarrassed attention given small favor deserve',\n",
       " 'cant already knew feel going disappointed entitled even sound help though',\n",
       " 'sweet felt people remember mom see self would feeling rude husband',\n",
       " 'clever feel',\n",
       " 'breakfast feeling remorseful pill wa diet took',\n",
       " 'feel beaten really rather death weight believe loss though something ha concept currently experiencing change lifestyle',\n",
       " 'know feel next longer step lost',\n",
       " 'feeling awful',\n",
       " 'feel like act imagining believe live joyful would front make audience',\n",
       " 'forearm totally clean funny maybe sticky touched hand feeling became thought wa hair',\n",
       " 'moment feel around suddenly completely perfect love hate disappear people',\n",
       " 'time remember solely together interaction social wasnt larger progress got ready jealous realized little feeling ha network',\n",
       " 'toward feel lot like leonard known im quite adore cohen affectionate make',\n",
       " 'feel really tranquil right',\n",
       " 'yet feel searching sure impatient fully',\n",
       " 'feeling know tearful wa even though unloved',\n",
       " 'hurt feel nothing wanted like told sincere bother ive even everything',\n",
       " 'feel wednesday distracted lost like train write evening got home much thought challenge therefore',\n",
       " 'little feel stressed starting wa',\n",
       " 'feeling bit admit intimidated challenge',\n",
       " 'feeling printer due new buy sewing wa think stress machine vulnerable',\n",
       " 'yet feel read ashamed',\n",
       " 'teenaged face feel pimple morning girl prom hear devastated',\n",
       " 'feeling restless beautiful miserable morning woke sunday',\n",
       " 'href feel work talking http www people might n think bookmark unwelcome class fn april sometimes url rel',\n",
       " 'moment missgivings show need wear dont beloved feeling without mask',\n",
       " 'turn away feel frightened even head',\n",
       " 'mother dinner feel today totally like delicious go account something put buy supermarket money',\n",
       " 'week com terminando feel depois fine eight ano e um day',\n",
       " 'project uncertain feel',\n",
       " 'energy positive super feel every vital full day',\n",
       " 'world feel inconsistent human completely dissatisfied whole character',\n",
       " 'knew feel murder got innocent wa would hauled euan mean',\n",
       " 'path feel going worker lot etc somehow mention support made child ther co sadly partner experience experimented determined woman realize career',\n",
       " 'threatened feel blog wrote pretty like knew sorry chill girl fuckin say would offended insulted apologize school high',\n",
       " 'moment thing feel impacting talented end',\n",
       " 'good feeling find feel happy',\n",
       " 'passion week bit nostalgic feel ha blog wonder gone writing time',\n",
       " 'feeling im particularly chocolate cookie dangerous',\n",
       " 'sits feeling im close ha ok hand always',\n",
       " 'erupt mellow mean spite past likely pm warning im quite little feeling raging day',\n",
       " 'feel victim win innocent like',\n",
       " 'spent feel anything ehb bothered time together bit didnt quite would woman much thought since actually ow',\n",
       " 'feel wanting resolve grievance sincere',\n",
       " 'skin primer based feel attire moisturizer time regular attainable color yet thats tender oil point also along something look great normally day',\n",
       " 'feel ahead ride apprehensive',\n",
       " 'bothered feel',\n",
       " 'beloved hint feel era nouveau bracelet art',\n",
       " 'convenience stair take occasionally bow consecutive night late ease number lethargic feeling particularly mostly',\n",
       " 'feel unsafe insecure',\n",
       " 'feel iphone sort let internet honest must pathetic tv saying',\n",
       " 'little concern feel understand rejected guess help',\n",
       " 'feeling pounded pavement hot black shoe laced',\n",
       " 'feel first get stupid putting men really place baby male cry woman cool',\n",
       " 'someone going direction back hold time many past activity clue think often unsure favorite child rich telling life little feeling childhood',\n",
       " 'inside mill work room first long christmas elihu lost people time one lit tree meeting gentle lovely fine christmastime joined home adult took living large feeling darkly wa son friend stately',\n",
       " 'god feel called see everyday ha praying convinced',\n",
       " 'feel passionate really design love everything learning',\n",
       " 'moment feel brain damaged dis worst getting',\n",
       " 'usual going family adventurous terribly plus cant visiting im neglect completely meaning feeling business',\n",
       " 'car wa killed accident friend funeral age',\n",
       " 'fearful feel near',\n",
       " 'feeling ecstatic wondering wa',\n",
       " 'feel talking love last people many dont proplems really said month vain wa bitched',\n",
       " 'intensity feel writing one really think beloved losing expressed would spouse emotion',\n",
       " 'cant waiting news thing feel agony folk imagine sentimental',\n",
       " 'feel caring even make',\n",
       " 'messy feel',\n",
       " 'lethargic go feel next morning uninspired',\n",
       " 'insulted feel',\n",
       " 'little feel relationship boost romantic need',\n",
       " 'bodily talk started girlfriend didnt close intimate relationship want friend avoid',\n",
       " 'lexicon feel use elevated intelligent',\n",
       " 'world feel wronged',\n",
       " 'lethargic something like feel dressed completely lazy uncomposed',\n",
       " 'editor moment feel missed like director everyone nice tiphany teachable else make comment',\n",
       " 'interest feel anything genuinely write passionate least',\n",
       " 'hard genre working non legitimate analyze popular literary le doe book enjoy fiction something feeling read make shake novel',\n",
       " 'something feel agitated empty missing',\n",
       " 'helpless feel ignoring heartbreaking better realized',\n",
       " 'little hour feel dazed alarming since still high',\n",
       " 'joyful feel would',\n",
       " 'feel caring touch wa love food compassion feed like',\n",
       " 'terribly feel sweet email ignored',\n",
       " 'feeling offended deeply hurt big fact',\n",
       " 'feel rushed overwhelmed',\n",
       " 'feel blog finish romantic finally post',\n",
       " 'steal feel joshua amused customer kinda',\n",
       " 'feel peace eye presence belief sincerity thrilled',\n",
       " 'feel love anyone ever linus fucked dontknow really could cant relationship way went else thought never without',\n",
       " 'feel sense make successful',\n",
       " 'forgotten feeling im low',\n",
       " 'practically way feel happy every perfect right life like',\n",
       " 'something like feel suffer worthwhile produce',\n",
       " 'feeling see creative alternate version wa',\n",
       " 'bit feeling nostalgic today',\n",
       " 'feel show like taken many includes supporting decision life way increasing woman area making country change',\n",
       " 'dirty feel really street nobody bin sought trash like',\n",
       " 'lol feel new dont process slutty make friend',\n",
       " 'turn bit feel people reluctant',\n",
       " 'feel wake worthless absolutely',\n",
       " 'feel red getting full gorgeous like bottom curl lowlights natural transition look making extremely hair instead section ended',\n",
       " 'come feel year season next gonna rainy aching start',\n",
       " 'feel slightest like lapse glad recent inconvenience violent go reason want sanity know rampage still',\n",
       " 'feel first poop triumphant joke time coffee sentence one couple stare scratch write sit head computer screen blank consists pot drink usually',\n",
       " 'drop feel ecstatic normal excruciating thousand mile unhappy like high',\n",
       " 'world one feel read sitting horrific ashamed live made',\n",
       " 'helping sweet feel happy wa could spirit',\n",
       " 'feel eating love delicate really ugly fig dried look much prettier fresh',\n",
       " 'better id feel',\n",
       " 'im feel hgtv watch talented like',\n",
       " 'feel conscious brave whether order believe choice want never make',\n",
       " 'offended feel question like',\n",
       " 'strange handful bit probably feel love always friend describing',\n",
       " 'feel ignored wanted hated',\n",
       " 'feel express dominant carefree poet crazy gene really writer rather voice wonderful wa',\n",
       " 'feeling discontent covetousnes wonder jealous',\n",
       " 'lately sick life living tired feeling making entire disturbing kind low hopelessness',\n",
       " 'sweet feel covered alone beating hand rhythm half heart scar put remind oh chest',\n",
       " 'little feeling im rebellious guess',\n",
       " 'yet feel resolved depressed somehow',\n",
       " 'tender feel',\n",
       " 'little feeling already im stressed',\n",
       " 'feel favourite flavour responsible like see nowhere yet terpene acceptable',\n",
       " 'strange ever bit feeling blog never really writing felt stop',\n",
       " 'href feeling http groggy',\n",
       " 'lose feel happy gained finally wieght like gotten pound really place would ive acceptable',\n",
       " 'explains thing feel book use easy well',\n",
       " 'brought feel docile resigned',\n",
       " 'feel anything awake call breathing give cant alone beating keeping still',\n",
       " 'daily loving dead feel decade presence physically almost parent two though even',\n",
       " 'pretty jolly feel',\n",
       " 'helpless feel read old parent sad case son ignoring unhappy',\n",
       " 'feel reminds advance take humiliated find despite attractive ra beautiful body life towards try sexually husband make still',\n",
       " 'helpless alone feel kind',\n",
       " 'feel totally ignored excluded',\n",
       " 'something feeling fantabulous im pissed seriously utterly stupid',\n",
       " 'wouldnt feel cheated died hey wish could say didnt accomplish tomorrow life something regretful know',\n",
       " 'lie feel like could frankly act way look know smug lazy ness',\n",
       " 'go feel empty home',\n",
       " 'something news feel share brave girl enough finally tell',\n",
       " 'feel prey defeated lion like',\n",
       " 'feeling liked like',\n",
       " 'wont feel personal hope u like excited rushed',\n",
       " 'secure bible feel safe verse space help transcend surround',\n",
       " 'know feel melbourne return hesitant awkwardly',\n",
       " 'little face feel prepared gloomy throw optimistic better way life le',\n",
       " 'lie wont feel ill inadequate best thats im worried heavenly want little job father nervous',\n",
       " 'feel finally slough hour past enveloped thirty ready invigorated face ha carapace odd crud day',\n",
       " 'good loving happy girl really type im busy glasgow feeling thing ive ha new right start',\n",
       " 'mother leadingstrings kept',\n",
       " 'feel ill spending constraint specific needing time find mind set want event wanting item pressured end',\n",
       " 'feel meditating written lord today morning care tender closing journal prayer may greatness psalm',\n",
       " 'feeling wa actually depressed inspired',\n",
       " 'feel actually pretty people age like come think gone devastated enough',\n",
       " 'pajama feeling around get home grouchy laze',\n",
       " 'feeling homesick weekend pretty',\n",
       " 'gonna optimistic leader coz started driven really paper meaning way feeling teach wa',\n",
       " 'study feel get possibly need like best listless even right',\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4370a232",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X,Y, test_size=0.2, random_state=42, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e3ff22f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    5362\n",
       "1    4665\n",
       "2    2159\n",
       "5    1937\n",
       "3    1304\n",
       "4     572\n",
       "Name: emotion, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"emotion\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31d1465d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7084192762047627\n",
      "0.8650540658791175\n",
      "0.9184949059316208\n",
      "0.9642477654853429\n",
      "0.87892993312082\n",
      "0.6648540533783361\n",
      "15999\n"
     ]
    }
   ],
   "source": [
    "one = df[df[\"emotion\"]==1][\"emotion\"].count()\n",
    "two = df[df[\"emotion\"]==2][\"emotion\"].count()\n",
    "three = df[df[\"emotion\"]==3][\"emotion\"].count()\n",
    "four = df[df[\"emotion\"]==4][\"emotion\"].count()\n",
    "five = df[df[\"emotion\"]==5][\"emotion\"].count()\n",
    "six = df[df[\"emotion\"]==6][\"emotion\"].count()\n",
    "total = len(df[\"emotion\"])\n",
    "\n",
    "weight_one = (total-one)/total\n",
    "\n",
    "weight_two = (total-two)/total\n",
    "\n",
    "weight_three = (total-three)/total\n",
    "\n",
    "weight_four = (total-four)/total\n",
    "\n",
    "weight_five = (total-five)/total\n",
    "\n",
    "weight_six= (total-six)/total\n",
    "\n",
    "\n",
    "print(weight_one)\n",
    "print(weight_two)\n",
    "print(weight_three)\n",
    "print(weight_four)\n",
    "print(weight_five)\n",
    "print(weight_six)\n",
    "\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0eb3fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d9731f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV 1/5] END max_depth=5, min_samples_leaf=5, min_samples_split=1, n_estimators=1900;, score=nan total time=   0.4s\n",
      "[CV 2/5] END max_depth=5, min_samples_leaf=5, min_samples_split=1, n_estimators=1900;, score=nan total time=   0.4s\n",
      "[CV 3/5] END max_depth=5, min_samples_leaf=5, min_samples_split=1, n_estimators=1900;, score=nan total time=   0.4s\n",
      "[CV 4/5] END max_depth=5, min_samples_leaf=5, min_samples_split=1, n_estimators=1900;, score=nan total time=   0.5s\n",
      "[CV 5/5] END max_depth=5, min_samples_leaf=5, min_samples_split=1, n_estimators=1900;, score=nan total time=   0.4s\n",
      "[CV 1/5] END max_depth=1, min_samples_leaf=4, min_samples_split=1, n_estimators=1500;, score=nan total time=   0.3s\n",
      "[CV 2/5] END max_depth=1, min_samples_leaf=4, min_samples_split=1, n_estimators=1500;, score=nan total time=   0.3s\n",
      "[CV 3/5] END max_depth=1, min_samples_leaf=4, min_samples_split=1, n_estimators=1500;, score=nan total time=   0.4s\n",
      "[CV 4/5] END max_depth=1, min_samples_leaf=4, min_samples_split=1, n_estimators=1500;, score=nan total time=   0.3s\n",
      "[CV 5/5] END max_depth=1, min_samples_leaf=4, min_samples_split=1, n_estimators=1500;, score=nan total time=   0.3s\n",
      "[CV 1/5] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=100;, score=0.336 total time=   0.5s\n",
      "[CV 2/5] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=100;, score=0.337 total time=   0.5s\n",
      "[CV 3/5] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=100;, score=0.339 total time=   0.4s\n",
      "[CV 4/5] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=100;, score=0.335 total time=   0.4s\n",
      "[CV 5/5] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=100;, score=0.336 total time=   0.4s\n",
      "[CV 1/5] END max_depth=2, min_samples_leaf=1, min_samples_split=1, n_estimators=1900;, score=nan total time=   0.4s\n",
      "[CV 2/5] END max_depth=2, min_samples_leaf=1, min_samples_split=1, n_estimators=1900;, score=nan total time=   0.5s\n",
      "[CV 3/5] END max_depth=2, min_samples_leaf=1, min_samples_split=1, n_estimators=1900;, score=nan total time=   0.4s\n",
      "[CV 4/5] END max_depth=2, min_samples_leaf=1, min_samples_split=1, n_estimators=1900;, score=nan total time=   0.4s\n",
      "[CV 5/5] END max_depth=2, min_samples_leaf=1, min_samples_split=1, n_estimators=1900;, score=nan total time=   0.4s\n",
      "[CV 1/5] END max_depth=2, min_samples_leaf=1, min_samples_split=1, n_estimators=1500;, score=nan total time=   0.3s\n",
      "[CV 2/5] END max_depth=2, min_samples_leaf=1, min_samples_split=1, n_estimators=1500;, score=nan total time=   0.3s\n",
      "[CV 3/5] END max_depth=2, min_samples_leaf=1, min_samples_split=1, n_estimators=1500;, score=nan total time=   0.3s\n",
      "[CV 4/5] END max_depth=2, min_samples_leaf=1, min_samples_split=1, n_estimators=1500;, score=nan total time=   0.4s\n",
      "[CV 5/5] END max_depth=2, min_samples_leaf=1, min_samples_split=1, n_estimators=1500;, score=nan total time=   0.3s\n",
      "[CV 1/5] END max_depth=1, min_samples_leaf=2, min_samples_split=5, n_estimators=1300;, score=0.335 total time=   4.5s\n",
      "[CV 2/5] END max_depth=1, min_samples_leaf=2, min_samples_split=5, n_estimators=1300;, score=0.335 total time=   4.0s\n",
      "[CV 3/5] END max_depth=1, min_samples_leaf=2, min_samples_split=5, n_estimators=1300;, score=0.335 total time=   3.8s\n",
      "[CV 4/5] END max_depth=1, min_samples_leaf=2, min_samples_split=5, n_estimators=1300;, score=0.335 total time=   4.2s\n",
      "[CV 5/5] END max_depth=1, min_samples_leaf=2, min_samples_split=5, n_estimators=1300;, score=0.335 total time=   4.0s\n",
      "[CV 1/5] END max_depth=4, min_samples_leaf=4, min_samples_split=5, n_estimators=1300;, score=0.335 total time=   8.2s\n",
      "[CV 2/5] END max_depth=4, min_samples_leaf=4, min_samples_split=5, n_estimators=1300;, score=0.335 total time=   8.1s\n",
      "[CV 3/5] END max_depth=4, min_samples_leaf=4, min_samples_split=5, n_estimators=1300;, score=0.335 total time=   8.2s\n",
      "[CV 4/5] END max_depth=4, min_samples_leaf=4, min_samples_split=5, n_estimators=1300;, score=0.335 total time=   8.2s\n",
      "[CV 5/5] END max_depth=4, min_samples_leaf=4, min_samples_split=5, n_estimators=1300;, score=0.335 total time=   8.2s\n",
      "[CV 1/5] END max_depth=4, min_samples_leaf=3, min_samples_split=1, n_estimators=700;, score=nan total time=   0.1s\n",
      "[CV 2/5] END max_depth=4, min_samples_leaf=3, min_samples_split=1, n_estimators=700;, score=nan total time=   0.1s\n",
      "[CV 3/5] END max_depth=4, min_samples_leaf=3, min_samples_split=1, n_estimators=700;, score=nan total time=   0.1s\n",
      "[CV 4/5] END max_depth=4, min_samples_leaf=3, min_samples_split=1, n_estimators=700;, score=nan total time=   0.1s\n",
      "[CV 5/5] END max_depth=4, min_samples_leaf=3, min_samples_split=1, n_estimators=700;, score=nan total time=   0.1s\n",
      "[CV 1/5] END max_depth=1, min_samples_leaf=5, min_samples_split=4, n_estimators=100;, score=0.335 total time=   0.2s\n",
      "[CV 2/5] END max_depth=1, min_samples_leaf=5, min_samples_split=4, n_estimators=100;, score=0.335 total time=   0.2s\n",
      "[CV 3/5] END max_depth=1, min_samples_leaf=5, min_samples_split=4, n_estimators=100;, score=0.335 total time=   0.2s\n",
      "[CV 4/5] END max_depth=1, min_samples_leaf=5, min_samples_split=4, n_estimators=100;, score=0.335 total time=   0.2s\n",
      "[CV 5/5] END max_depth=1, min_samples_leaf=5, min_samples_split=4, n_estimators=100;, score=0.335 total time=   0.2s\n",
      "[CV 1/5] END max_depth=1, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.335 total time=   0.8s\n",
      "[CV 2/5] END max_depth=1, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.335 total time=   0.8s\n",
      "[CV 3/5] END max_depth=1, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.335 total time=   0.8s\n",
      "[CV 4/5] END max_depth=1, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.335 total time=   0.9s\n",
      "[CV 5/5] END max_depth=1, min_samples_leaf=3, min_samples_split=3, n_estimators=300;, score=0.335 total time=   0.9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(estimator=RandomForestClassifier(),\n",
       "                   param_distributions={'max_depth': [1, 2, 3, 4, 5],\n",
       "                                        'min_samples_leaf': [1, 2, 3, 4, 5],\n",
       "                                        'min_samples_split': [1, 2, 3, 4, 5],\n",
       "                                        'n_estimators': [100, 300, 500, 700,\n",
       "                                                         900, 1100, 1300, 1500,\n",
       "                                                         1700, 1900]},\n",
       "                   verbose=5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "param ={ \"n_estimators\":[x for x in range(100,2000, 200)],\n",
    "         \"max_depth\" : [x for x in range(1,6)],\n",
    "         \"min_samples_split\" : [x for x in range(1,6)],\n",
    "         \"min_samples_leaf\" : [x for x in range(1,6)],\n",
    "       }\n",
    "\n",
    "rf_cv = RandomizedSearchCV(rf, param_distributions=param, verbose=5)\n",
    "\n",
    "rf_cv.fit(tfidf_matrix, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b45f242f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 100,\n",
       " 'min_samples_split': 3,\n",
       " 'min_samples_leaf': 4,\n",
       " 'max_depth': 3}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_para = rf_cv.best_params_\n",
    "best_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78e60e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': [50, 60, 70, 80, 90, 100, 110, 120, 130, 140],\n",
       " 'min_samples_split': [4, 3, 2],\n",
       " 'min_samples_leaf': [5, 4, 6],\n",
       " 'max_depth': [3]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params ={'n_estimators': [x for x in range(best_para[\"n_estimators\"]-50,  best_para[\"n_estimators\"]+50, 10)],\n",
    "         'min_samples_split': [best_para[\"min_samples_split\"]+1, best_para[\"min_samples_split\"], best_para[\"min_samples_split\"]-1],\n",
    "         'min_samples_leaf': [best_para[\"min_samples_leaf\"]+1, best_para[\"min_samples_leaf\"], best_para[\"min_samples_leaf\"]+2],\n",
    "         'max_depth': [best_para[\"max_depth\"]]}\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74fd530d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=50; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=60; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=60; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=60; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=60; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=60; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=80; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=80; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=80; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=80; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=80; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=90; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=90; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=90; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=90; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=90; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=110; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=110; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=110; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=110; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=110; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=120; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=120; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=120; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=120; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=120; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=130; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=130; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=130; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=130; total time=   0.7s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=130; total time=   0.7s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=140; total time=   0.7s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=140; total time=   0.7s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=140; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=140; total time=   0.7s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=4, n_estimators=140; total time=   0.7s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=60; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=60; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=60; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=60; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=60; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=80; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=80; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=80; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=80; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=80; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=90; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=90; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=90; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=90; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=90; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=100; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=100; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=100; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=100; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=110; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=110; total time=   0.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=110; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=110; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=110; total time=   0.7s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=120; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=120; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=120; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=120; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=120; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=130; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=130; total time=   0.7s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=130; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=130; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=130; total time=   0.7s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=140; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=140; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=140; total time=   0.7s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=140; total time=   0.7s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=3, n_estimators=140; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=60; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=60; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=60; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=60; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=60; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=80; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=80; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=80; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=80; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=80; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=90; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=90; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=90; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=90; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=90; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=100; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=100; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=100; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=110; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=110; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=110; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=110; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=110; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=120; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=120; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=120; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=120; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=120; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=130; total time=   0.7s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=130; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=130; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=130; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=130; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=140; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=140; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=140; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=140; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=5, min_samples_split=2, n_estimators=140; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=60; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=60; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=60; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=60; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=60; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=70; total time=   0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=80; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=80; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=80; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=80; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=80; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=90; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=90; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=90; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=90; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=90; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=110; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=110; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=110; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=110; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=110; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=120; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=120; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=120; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=120; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=120; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=130; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=130; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=130; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=130; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=130; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=140; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=140; total time=   0.9s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=140; total time=   0.9s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=140; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=4, n_estimators=140; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=60; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=60; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=60; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=60; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=60; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=80; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=80; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=80; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=80; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=80; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=90; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=90; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=90; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=90; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=90; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=110; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=110; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=110; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=110; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=110; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=120; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=120; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=120; total time=   0.7s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=120; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=120; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=130; total time=   0.8s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=130; total time=   0.7s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=130; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=130; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=130; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=140; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=140; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=140; total time=   0.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=140; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=3, n_estimators=140; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=60; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=60; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=60; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=60; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=60; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=80; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=80; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=80; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=80; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=80; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=90; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=90; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=90; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=90; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=90; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=110; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=110; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=110; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=110; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=110; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=120; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=120; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=120; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=120; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=120; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=130; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=130; total time=   0.7s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=130; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=130; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=130; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=140; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=140; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=140; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=140; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=140; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=60; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=60; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=60; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=60; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=60; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=80; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=80; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=80; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=80; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=80; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=90; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=90; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=90; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=90; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=90; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=110; total time=   0.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=110; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=110; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=110; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=110; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=120; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=120; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=120; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=120; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=120; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=130; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=130; total time=   0.7s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=130; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=130; total time=   0.7s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=130; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=140; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=140; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=140; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=140; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=4, n_estimators=140; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=60; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=60; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=60; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=60; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=60; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=80; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=80; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=80; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=80; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=80; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=90; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=90; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=90; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=90; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=90; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=110; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=110; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=110; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=110; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=110; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=120; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=120; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=120; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=120; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=120; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=130; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=130; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=130; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=130; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=130; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=140; total time=   0.7s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=140; total time=   0.7s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=140; total time=   0.7s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=140; total time=   0.7s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=3, n_estimators=140; total time=   0.7s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=50; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=60; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=60; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=60; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=60; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=60; total time=   0.2s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=70; total time=   0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=70; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=80; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=80; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=80; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=80; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=80; total time=   0.3s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=90; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=90; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=90; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=90; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=90; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=100; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=100; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=110; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=110; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=110; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=110; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=110; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=120; total time=   0.5s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=120; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=120; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=120; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=120; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=130; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=130; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=130; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=130; total time=   0.7s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=130; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=140; total time=   0.7s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=140; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=140; total time=   0.7s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=140; total time=   0.6s\n",
      "[CV] END max_depth=3, min_samples_leaf=6, min_samples_split=2, n_estimators=140; total time=   0.7s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=RandomForestClassifier(),\n",
       "             param_grid={'max_depth': [3], 'min_samples_leaf': [5, 4, 6],\n",
       "                         'min_samples_split': [4, 3, 2],\n",
       "                         'n_estimators': [50, 60, 70, 80, 90, 100, 110, 120,\n",
       "                                          130, 140]},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "gr_cv = GridSearchCV(rf, param_grid=params, verbose=2)\n",
    "\n",
    "gr_cv.fit(tfidf_matrix, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ffa03ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 3,\n",
       " 'min_samples_leaf': 4,\n",
       " 'min_samples_split': 3,\n",
       " 'n_estimators': 60}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88765726",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()),\n",
    "    (\"classifier\", RandomForestClassifier( \n",
    "                                          #class_weight={1:weight_one,2:weight_two,3:weight_three,4:weight_four,5:weight_five,6:weight_six}\n",
    "                                         ))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34521887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                ('classifier', RandomForestClassifier())])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d326d834",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pip.predict(x_test)\n",
    "\n",
    "y_pred_prob = pip.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb513d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8821875"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "62cb60a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1 , 0.28, 0.  , 0.  , 0.29, 0.33]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = preprocessing_text(\"how tasty\")\n",
    "\n",
    "pip.predict_proba([q])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "39caaf76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sadness': 1, 'anger': 2, 'love': 3, 'surprise': 4, 'fear': 5, 'joy': 6}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "({'sadness':1, 'anger':2, 'love':3, 'surprise':4, 'fear':5, 'joy':6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffe854d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
